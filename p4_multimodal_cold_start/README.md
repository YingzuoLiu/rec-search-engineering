# Multimodal Item Representation for Cold-Start Recommendation

**Tech Stack:** PyTorch Â· CLIP Â· FAISS  
**Dataset:** Amazon Product Images + Titles + Metadata

---

## ğŸ“Œ Project Motivation

In real-world search and recommendation systems, **cold-start items** (new products, new merchants, newly listed content) often suffer from poor exposure due to the lack of user interaction history.

This project focuses on **item-side cold start**, exploring how **multimodal signals (image + text)** can be leveraged to build robust item representations before any behavioral data is available.

---

## ğŸ§  Where Multimodal Models Fit in the Search Pipeline

This project targets the **Recall / Candidate Generation** stage of a search or recommendation system.

```
User Query
   â†“
Query Understanding
   â†“
ğŸ”¹ Multimodal Recall (This Project)
   â†“
Lightweight Ranking
   â†“
Serving
```

### Why here?

- Cold-start items cannot rely on collaborative signals.
- Multimodal embeddings allow **semantic recall** even with zero interactions.
- FAISS enables **low-latency ANN retrieval** at scale.

This project intentionally **does not place LLMs in ranking or generation**, keeping inference cost and latency under control.

---

## ğŸ§© Approach Overview

### 1. Multimodal Item Encoding

Each item is represented using:

- **Image:** Product image
- **Text:** Title + structured metadata

We use **CLIP** to encode both modalities into a **shared embedding space**, enabling semantic alignment between visual and textual information.

```
Image â”€â”
       â”œâ”€ CLIP Encoder â”€â†’ Item Embedding
Text  â”€â”˜
```

### 2. Vector Indexing with FAISS

- Item embeddings are indexed using **FAISS ANN search**
- Enables fast similarity-based retrieval for:
  - text queries
  - image queries
  - hybrid search scenarios

This setup simulates **production-style recall infrastructure**.

---

## ğŸ” Evaluation Strategy

Since cold-start items lack interaction data, evaluation focuses on **retrieval robustness** rather than CTR.

### Baselines

- Text-only embedding
- Multimodal (Image + Text) embedding

### Metrics

- Recall@K on held-out cold-start items
- Qualitative retrieval inspection

### Findings

- Multimodal embeddings show **more stable recall** for visually distinctive items.
- Text-only models degrade when titles are short or ambiguous.
- Image signals provide **complementary semantic cues**.

---

## ğŸ§ª Ablation Study

To better understand modality contribution, we conduct a small-scale ablation:

| Setting | Observation |
|---------|-------------|
| Image only | Strong for appearance-driven items |
| Text only | Sensitive to sparse or noisy titles |
| Image + Text | Most robust overall |

This confirms that **multimodal fusion improves cold-start recall consistency**, rather than maximizing a single metric.

---

## ğŸ” Applicability to Food & Merchant Discovery

This approach naturally extends to:

- Food discovery (dish photos + descriptions)
- Merchant onboarding (store images + profiles)
- Local discovery platforms

In such scenarios:

- Visual cues are often as important as text
- Cold-start is frequent and unavoidable

---

## âš–ï¸ Design Trade-offs

### Pros

- No dependency on user behavior
- Strong cold-start performance
- Scales well with FAISS

### Cons

- Multimodal encoders increase offline compute cost
- Requires careful feature hygiene (image quality, text noise)

---

## ğŸš€ Future Extensions

- Late fusion with behavior-based embeddings once data becomes available
- Query-side multimodal understanding
- Lightweight re-ranking on top of multimodal recall

---

## âœ… Key Takeaway

This project demonstrates how **multimodal representation learning** can be cleanly integrated into the **recall layer** of a search/recommendation system to mitigate cold-start issuesâ€”**without introducing heavy LLM dependencies or latency risks**.

## Limitations & Future Work

**Limitations**
- æœ¬é¡¹ç›®ä¸»è¦éªŒè¯å¤šæ¨¡æ€ä¿¡æ¯åœ¨å†·å¯åŠ¨å¬å›åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå½“å‰èåˆæ–¹å¼åŸºäº CLIP æä¾›çš„ç»Ÿä¸€ embedding ç©ºé—´ï¼Œæœªå¯¹ä¸åŒæ¨¡æ€çš„é‡è¦æ€§è¿›è¡Œæ ·æœ¬çº§å»ºæ¨¡ã€‚
- å¯¹äºå›¾ç‰‡ç¼ºå¤±æˆ–è´¨é‡è¾ƒä½çš„å•†å“ï¼Œå¤šæ¨¡æ€è¡¨ç¤ºçš„ç¨³å®šæ€§ä»ä¾èµ–æ–‡æœ¬ä¸åŸºç¡€å…ƒæ•°æ®è·¯å¾„ä½œä¸ºéšå¼ fallbackã€‚
- å®éªŒè§„æ¨¡ç›¸å¯¹æœ‰é™ï¼Œç»“æœæ›´ä¾§é‡æ–¹æ³•å¯è¡Œæ€§éªŒè¯ï¼Œè€Œéç»™å‡ºå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„å·¥ä¸šçº§ç»“è®ºã€‚

**Future Work**
- åœ¨ä¿æŒå¬å›é˜¶æ®µä½å»¶è¿Ÿçš„å‰æä¸‹ï¼Œå¼•å…¥ gating æˆ–æ¡ä»¶èåˆæœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å•†å“ç‰¹å¾è‡ªé€‚åº”è°ƒæ•´ä¸åŒæ¨¡æ€çš„è´¡çŒ®ã€‚
- åœ¨æ›´è´´è¿‘çœŸå®ä¸šåŠ¡åˆ†å¸ƒçš„æ•°æ®ä¸Šï¼Œè¯„ä¼°å¤šæ¨¡æ€å¬å›å¯¹å†·å¯åŠ¨å•†å“æ›å…‰ä¸ç‚¹å‡»è¡Œä¸ºçš„é•¿æœŸå½±å“ã€‚
- è®¾è®¡æ˜¾å¼çš„æ¨¡æ€é™çº§ä¸è·¯ç”±ç­–ç•¥ï¼Œæé«˜ç³»ç»Ÿåœ¨æ¨¡æ€ç¼ºå¤±æˆ–å™ªå£°åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚
